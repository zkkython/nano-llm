{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "class SelfAttention(nn.Module):\n",
    "  \n",
    "    def __init__(self\n",
    "                 ,d_model: int\n",
    "                 ,row_dim = 0\n",
    "                 ,col_dim = 1):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.row_dim = row_dim\n",
    "        self.col_dim = col_dim\n",
    "        self.W_q = nn.Linear(in_features= d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features= d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features= d_model, out_features=d_model, bias=False)\n",
    "    \n",
    "    def forward(self, token_encodings):\n",
    "        q = self.W_q(token_encodings)\n",
    "        k = self.W_k(token_encodings)\n",
    "        v = self.W_v(token_encodings)\n",
    "\n",
    "        sims = torch.matmul(q, k.transpose(self.row_dim, self.col_dim))\n",
    "        scaled_sims = sims / math.sqrt(self.d_model)\n",
    "        #scaled_sims = sims / torch.tensor(k.size(self.col_dim) ** 0.5)\n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor([[1.0100, 1.0641],\n",
    "        [0.2040, 0.7057],\n",
    "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0100, 1.0641],\n",
       "        [0.2040, 0.7057],\n",
       "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "encoding_matrix = torch.tensor([\n",
    "    [1.16, 0.23],\n",
    "    [0.57, 1.36],\n",
    "    [4.41, -2.16]\n",
    "], dtype=torch.float32\n",
    ")\n",
    "\n",
    "selfAttention = SelfAttention(d_model=2, row_dim=0, col_dim=1)\n",
    "selfAttention(encoding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现Masked Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6038,  0.7434],\n",
       "        [-0.0062,  0.6072],\n",
       "        [ 3.4989,  2.2427]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "class MaskedSelfAttention(nn.Module):\n",
    "  \n",
    "    def __init__(self\n",
    "                 ,d_model: int\n",
    "                 ,row_dim = 0\n",
    "                 ,col_dim = 1):\n",
    "        super(MaskedSelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.row_dim = row_dim\n",
    "        self.col_dim = col_dim\n",
    "        self.W_q = nn.Linear(in_features= d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features= d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features= d_model, out_features=d_model, bias=False)\n",
    "    \n",
    "    def forward(self, token_encodings, mask=None):\n",
    "        q = self.W_q(token_encodings)\n",
    "        k = self.W_k(token_encodings)\n",
    "        v = self.W_v(token_encodings)\n",
    "\n",
    "        sims = torch.matmul(q, k.transpose(self.row_dim, self.col_dim))\n",
    "        scaled_sims = sims / math.sqrt(self.d_model)\n",
    "        if mask is not None:\n",
    "            scaled_sims = scaled_sims.masked_fill(mask = mask, value = -1e9)\n",
    "        #scaled_sims = sims / torch.tensor(k.size(self.col_dim) ** 0.5)\n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "        return attention_scores\n",
    "    \n",
    "    \n",
    "torch.manual_seed(42)\n",
    "\n",
    "encoding_matrix = torch.tensor([\n",
    "    [1.16, 0.23],\n",
    "    [0.57, 1.36],\n",
    "    [4.41, -2.16]\n",
    "], dtype=torch.float32\n",
    ")\n",
    "\n",
    "mask = torch.tril(torch.ones(encoding_matrix.size(0), encoding_matrix.size(0))).bool()\n",
    "mask = mask == 0\n",
    "\n",
    "maskedSelfAttention = MaskedSelfAttention(d_model=2, row_dim=0, col_dim=1)\n",
    "maskedSelfAttention(encoding_matrix, mask = mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0100, 1.0641],\n",
       "        [0.2040, 0.7057],\n",
       "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "class Attention(nn.Module):\n",
    "  \n",
    "    def __init__(self\n",
    "                 ,d_model: int\n",
    "                 ,row_dim = 0\n",
    "                 ,col_dim = 1\n",
    "                 ):\n",
    "        super(Attention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.row_dim = row_dim\n",
    "        self.col_dim = col_dim\n",
    "        self.W_q = nn.Linear(in_features= d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features= d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features= d_model, out_features=d_model, bias=False)\n",
    "    \n",
    "    def forward(self, token_encodings_q,token_encodings_k,token_encodings_v, mask=None):\n",
    "        q = self.W_q(token_encodings_q)\n",
    "        k = self.W_k(token_encodings_k)\n",
    "        v = self.W_v(token_encodings_v)\n",
    "\n",
    "        sims = torch.matmul(q, k.transpose(self.row_dim, self.col_dim))\n",
    "        scaled_sims = sims / math.sqrt(self.d_model)\n",
    "        if mask is not None:\n",
    "            scaled_sims = scaled_sims.masked_fill(mask = mask, value = -1e9)\n",
    "        #scaled_sims = sims / torch.tensor(k.size(self.col_dim) ** 0.5)\n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "        return attention_scores\n",
    "    \n",
    "\n",
    "encoding_for_q = torch.tensor([\n",
    "    [1.16, 0.23],\n",
    "    [0.57, 1.36],\n",
    "    [4.41, -2.16]\n",
    "], dtype=torch.float32\n",
    ")\n",
    "\n",
    "encoding_for_k = torch.tensor([\n",
    "    [1.16, 0.23],\n",
    "    [0.57, 1.36],\n",
    "    [4.41, -2.16]\n",
    "], dtype=torch.float32\n",
    ")\n",
    "\n",
    "encoding_for_v = torch.tensor([\n",
    "    [1.16, 0.23],\n",
    "    [0.57, 1.36],\n",
    "    [4.41, -2.16]\n",
    "], dtype=torch.float32\n",
    ") \n",
    "\n",
    "torch.manual_seed(42)\n",
    "attention = Attention(d_model=2, row_dim=0, col_dim=1)\n",
    "attention(encoding_for_q, encoding_for_k, encoding_for_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0100, 1.0641],\n",
       "        [0.2040, 0.7057],\n",
       "        [3.4989, 2.2427]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self\n",
    "                 ,d_model: int\n",
    "                 ,num_heads: int\n",
    "                 ,row_dim = 0\n",
    "                 ,col_dim = 1\n",
    "                ):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.row_dim = row_dim\n",
    "        self.col_dim = col_dim\n",
    "        \n",
    "        self.heads = [\n",
    "            Attention(d_model=d_model, row_dim=row_dim, col_dim=col_dim) for _ in range(num_heads)\n",
    "        ]\n",
    "    \n",
    "    def forward(self, token_encodings_q,token_encodings_k,token_encodings_v, mask=None):\n",
    "        attention_scores_list = []\n",
    "        for head in self.heads:\n",
    "            attention_scores = head(token_encodings_q, token_encodings_k, token_encodings_v, mask)\n",
    "            attention_scores_list.append(attention_scores)\n",
    "        attention_scores = torch.cat(attention_scores_list, dim=self.col_dim)\n",
    "        return attention_scores\n",
    "\n",
    "encoding_for_q = torch.tensor([\n",
    "    [1.16, 0.23],\n",
    "    [0.57, 1.36],\n",
    "    [4.41, -2.16]\n",
    "], dtype=torch.float32\n",
    ")\n",
    "\n",
    "encoding_for_k = torch.tensor([\n",
    "    [1.16, 0.23],\n",
    "    [0.57, 1.36],\n",
    "    [4.41, -2.16]\n",
    "], dtype=torch.float32\n",
    ")\n",
    "\n",
    "encoding_for_v = torch.tensor([\n",
    "    [1.16, 0.23],\n",
    "    [0.57, 1.36],\n",
    "    [4.41, -2.16]\n",
    "], dtype=torch.float32\n",
    ") \n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "multiattention = MultiHeadAttention(d_model=2, num_heads=1, row_dim=0, col_dim=1)\n",
    "result = multiattention(encoding_for_q, encoding_for_k, encoding_for_v)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3267, -0.2788, -0.4220],\n",
       "        [-1.3323, -0.3639,  0.1513],\n",
       "        [-0.3514, -0.7906, -0.0915]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True],\n",
       "        [False, False,  True],\n",
       "        [False, False, False]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones(3, 3))\n",
    "mask = mask == 0\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
